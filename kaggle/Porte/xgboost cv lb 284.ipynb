{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ecf01439-91f9-40c2-90ca-cfc9fef897f9",
    "_uuid": "663762d0acd501a442290010ddc25eaea219ff13"
   },
   "source": [
    "Based on [olivier's script](https://www.kaggle.com/ogrellier/xgb-classifier-upsampling-lb-0-283)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "45ba73d4-6c4c-40bb-9390-7dfc956c555d",
    "_uuid": "dbd332f83c89108c4e641218f5c4e7b9cd325b80"
   },
   "outputs": [],
   "source": [
    "MAX_ROUNDS = 400\n",
    "OPTIMIZE_ROUNDS = False\n",
    "LEARNING_RATE = 0.07\n",
    "EARLY_STOPPING_ROUNDS = 50  \n",
    "# Note: I set EARLY_STOPPING_ROUNDS high so that (when OPTIMIZE_ROUNDS is set)\n",
    "#       I will get lots of information to make my own judgment.  You should probably\n",
    "#       reduce EARLY_STOPPING_ROUNDS if you want to do actual early stopping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7e199c98-16b0-45e7-a6d1-bdd9325c2631",
    "_uuid": "b277fe426336d65ac71f0e6ac96c7ee16d02074c"
   },
   "source": [
    "I recommend initially setting <code>MAX_ROUNDS</code> fairly high and using <code>OPTIMIZE_ROUNDS</code> to get an idea of the appropriate number of rounds (which, in my judgment, should be close to the maximum value of  <code>best_ntree_limit</code> among all folds, maybe even a bit higher if your model is adequately regularized...or alternatively, you could set <code>verbose=True</code> and look at the details to try to find a number of rounds that works well for all folds).  Then I would turn off <code>OPTIMIZE_ROUNDS</code> and set <code>MAX_ROUNDS</code> to the appropraite number of total rounds.  \n",
    "\n",
    "The problem with \"early stopping\" by choosing the best round for each fold is that it overfits to the validation data.    It's therefore liable not to produce the optimal model for predicting test data, and if it's used to produce validation data for stacking/ensembling with other models, it would cause this one to have too much weight in the ensemble.  Another possibility (and the default for XGBoost, it seems) is to use the round where the early stop actually happens (with the lag that verifies lack of improvement) rather than the best round.  That solves the overfitting problem (provided the lag is long enough), but so far it doesn't seem to have helped.  (I got a worse validation score with 20-round early stopping per fold than with a constant number of rounds for all folds, so the early stopping actually seemed to underfit.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b7258128-55f9-4543-8611-5e0a6661837b",
    "_uuid": "72171ee53e170096d37a18eef84682fa348ae5c4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from numba import jit\n",
    "import time\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "3d16f16e-12cc-4b41-b7bd-fa05ce44770c",
    "_uuid": "154b078a7e86c0a5a328118a61d28e2581bb3b0a"
   },
   "outputs": [],
   "source": [
    "# Compute gini\n",
    "\n",
    "# from CPMP's kernel https://www.kaggle.com/cpmpml/extremely-fast-gini-computation\n",
    "@jit\n",
    "def eval_gini(y_true, y_prob):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_true = y_true[np.argsort(y_prob)]\n",
    "    ntrue = 0\n",
    "    gini = 0\n",
    "    delta = 0\n",
    "    n = len(y_true)\n",
    "    for i in range(n-1, -1, -1):\n",
    "        y_i = y_true[i]\n",
    "        ntrue += y_i\n",
    "        gini += y_i * delta\n",
    "        delta += 1 - y_i\n",
    "    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n",
    "    return gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "99b88ea4-a9af-45aa-9df0-86412d7264cf",
    "_uuid": "67a8ca9dead7110c776d7f75bb8963b3429617cb"
   },
   "outputs": [],
   "source": [
    "# Funcitons from olivier's kernel\n",
    "# https://www.kaggle.com/ogrellier/xgb-classifier-upsampling-lb-0-283\n",
    "\n",
    "# def gini_xgb(preds, dtrain):\n",
    "#     labels = dtrain.get_label()\n",
    "#     gini_score = -eval_gini(labels, preds)\n",
    "#     return [('gini', gini_score)]\n",
    "\n",
    "def target_encode(trn_series=None,    # Revised to encode validation series\n",
    "                  val_series=None,\n",
    "                  tst_series=None,\n",
    "                  target=None,\n",
    "                  min_samples_leaf=1,\n",
    "                  smoothing=1,\n",
    "                  noise_level=0):\n",
    "    \"\"\"\n",
    "    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n",
    "    https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf\n",
    "    trn_series : training categorical feature as a pd.Series\n",
    "    tst_series : test categorical feature as a pd.Series\n",
    "    target : target data as a pd.Series\n",
    "    min_samples_leaf (int) : minimum samples to take category average into account\n",
    "    smoothing (int) : smoothing effect to balance categorical average vs prior\n",
    "    \"\"\"\n",
    "    assert len(trn_series) == len(target)\n",
    "    assert trn_series.name == tst_series.name\n",
    "    temp = pd.concat([trn_series, target], axis=1)\n",
    "    \n",
    "    # Compute target mean\n",
    "    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n",
    "    \n",
    "    # Compute smoothing\n",
    "    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n",
    "    \n",
    "    # Apply average function to all target data\n",
    "    prior = target.mean()\n",
    "    \n",
    "    # The bigger the count the less full_avg is taken into account\n",
    "    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n",
    "    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n",
    "    \n",
    "    # Apply averages to trn and tst series\n",
    "    ft_trn_series = pd.merge(\n",
    "        left = trn_series.to_frame(trn_series.name),\n",
    "        right = averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on = trn_series.name, # index name\n",
    "        how = 'left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    \n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_trn_series.index = trn_series.index # index copy\n",
    "        # trn_series processing end\n",
    "    \n",
    "    ft_val_series = pd.merge(\n",
    "        left = val_series.to_frame(val_series.name),\n",
    "        right = averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on = val_series.name,\n",
    "        how = 'left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_val_series.index = val_series.index\n",
    "        # val_series processing end\n",
    "        \n",
    "    ft_tst_series = pd.merge(\n",
    "        left = tst_series.to_frame(tst_series.name),\n",
    "        right = averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on = tst_series.name,\n",
    "        how = 'left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_tst_series.index = tst_series.index\n",
    "        # tst_series processing end\n",
    "        \n",
    "    return add_noise(ft_trn_series, noise_level), add_noise(ft_val_series, noise_level), add_noise(ft_tst_series, noise_level)\n",
    "\n",
    "def add_noise(series, noise_level):\n",
    "    return series * (1 + noise_level * np.random.randn(len(series)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "52b50086-b405-4598-b11c-97887cdcce8e",
    "_uuid": "07a5a5782894611e9006ae1b399b0b8fb8a0f06b"
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "train_df = pd.read_csv('./input/train.csv', na_values=\"-1\") # .iloc[0:200,:]\n",
    "test_df = pd.read_csv('./input/test.csv', na_values=\"-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "b9e041d2-18fb-4a8a-8bb4-577e8993189b",
    "_uuid": "2ac19052e2f2c14d79962af5e5a8ee3d54a28695"
   },
   "outputs": [],
   "source": [
    "# from olivier\n",
    "train_features = [\n",
    "    \"ps_car_13\",  #            : 1571.65 / shadow  609.23\n",
    "\t\"ps_reg_03\",  #            : 1408.42 / shadow  511.15\n",
    "\t\"ps_ind_05_cat\",  #        : 1387.87 / shadow   84.72\n",
    "\t\"ps_ind_03\",  #            : 1219.47 / shadow  230.55\n",
    "\t\"ps_ind_15\",  #            :  922.18 / shadow  242.00\n",
    "\t\"ps_reg_02\",  #            :  920.65 / shadow  267.50\n",
    "\t\"ps_car_14\",  #            :  798.48 / shadow  549.58\n",
    "\t\"ps_car_12\",  #            :  731.93 / shadow  293.62\n",
    "\t\"ps_car_01_cat\",  #        :  698.07 / shadow  178.72\n",
    "\t\"ps_car_07_cat\",  #        :  694.53 / shadow   36.35\n",
    "\t\"ps_ind_17_bin\",  #        :  620.77 / shadow   23.15\n",
    "\t\"ps_car_03_cat\",  #        :  611.73 / shadow   50.67\n",
    "\t\"ps_reg_01\",  #            :  598.60 / shadow  178.57\n",
    "\t\"ps_car_15\",  #            :  593.35 / shadow  226.43\n",
    "\t\"ps_ind_01\",  #            :  547.32 / shadow  154.58\n",
    "\t\"ps_ind_16_bin\",  #        :  475.37 / shadow   34.17\n",
    "\t\"ps_ind_07_bin\",  #        :  435.28 / shadow   28.92\n",
    "\t\"ps_car_06_cat\",  #        :  398.02 / shadow  212.43\n",
    "\t\"ps_car_04_cat\",  #        :  376.87 / shadow   76.98\n",
    "\t\"ps_ind_06_bin\",  #        :  370.97 / shadow   36.13\n",
    "\t\"ps_car_09_cat\",  #        :  214.12 / shadow   81.38\n",
    "\t\"ps_car_02_cat\",  #        :  203.03 / shadow   26.67\n",
    "\t\"ps_ind_02_cat\",  #        :  189.47 / shadow   65.68\n",
    "\t\"ps_car_11\",  #            :  173.28 / shadow   76.45\n",
    "\t\"ps_car_05_cat\",  #        :  172.75 / shadow   62.92\n",
    "\t\"ps_calc_09\",  #           :  169.13 / shadow  129.72\n",
    "\t\"ps_calc_05\",  #           :  148.83 / shadow  120.68\n",
    "\t\"ps_ind_08_bin\",  #        :  140.73 / shadow   27.63\n",
    "\t\"ps_car_08_cat\",  #        :  120.87 / shadow   28.82\n",
    "\t\"ps_ind_09_bin\",  #        :  113.92 / shadow   27.05\n",
    "\t\"ps_ind_04_cat\",  #        :  107.27 / shadow   37.43\n",
    "\t\"ps_ind_18_bin\",  #        :   77.42 / shadow   25.97\n",
    "\t\"ps_ind_12_bin\",  #        :   39.67 / shadow   15.52\n",
    "\t\"ps_ind_14\",  #            :   37.37 / shadow   16.65\n",
    "]\n",
    "# add combinations\n",
    "combs = [\n",
    "    ('ps_reg_01', 'ps_car_02_cat'),  \n",
    "    ('ps_reg_01', 'ps_car_04_cat'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "d9a217fa-50f4-43a7-805b-d1c796a7ebf7",
    "_uuid": "da09aaf7c0c77a131c7d9d53feae512c8f9730c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current feature                                 ps_reg_01_plus_ps_car_04_cat    2 in 0.04339"
     ]
    }
   ],
   "source": [
    "# Process data\n",
    "id_test = test_df['id'].values\n",
    "id_train = train_df['id'].values\n",
    "y = train_df['target']\n",
    "\n",
    "start = time.time()\n",
    "for n_c, (f1, f2) in enumerate(combs):\n",
    "    name1 = f1 + \"_plus_\" + f2\n",
    "    print('current feature %60s %4d in %5.5f'\n",
    "          % (name1, n_c + 1, (time.time() - start) / 60), end='')\n",
    "    print('\\r' * 75, end='')\n",
    "    train_df[name1] = train_df[f1].apply(lambda x: str(x)) + \"_\" + train_df[f2].apply(lambda x: str(x))\n",
    "    test_df[name1] = test_df[f1].apply(lambda x: str(x)) + \"_\" + test_df[f2].apply(lambda x: str(x))\n",
    "    # Label Encode\n",
    "    lbl = LabelEncoder()\n",
    "    lbl.fit(list(train_df[name1].values) + list(test_df[name1].values))\n",
    "    train_df[name1] = lbl.transform(list(train_df[name1].values))\n",
    "    test_df[name1] = lbl.transform(list(test_df[name1].values))\n",
    "\n",
    "    train_features.append(name1)\n",
    "    \n",
    "X = train_df[train_features]\n",
    "test_df = test_df[train_features]\n",
    "\n",
    "f_cats = [f for f in X.columns if \"_cat\" in f]\n",
    "# catagory columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "1b36eb15-ee01-43a3-8766-27650f98158d",
    "_uuid": "6255e3c12616b0279cef5c1bdec97751bb72d8b8"
   },
   "outputs": [],
   "source": [
    "y_valid_pred = 0*y\n",
    "\n",
    "y_test_pred = 0\n",
    "# initialization to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter :  Counter({0: 595212})\n",
      "0         0\n",
      "1         0\n",
      "2         0\n",
      "3         0\n",
      "4         0\n",
      "         ..\n",
      "595207    0\n",
      "595208    0\n",
      "595209    0\n",
      "595210    0\n",
      "595211    0\n",
      "Name: target, Length: 595212, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print('Counter : ',Counter(y_valid_pred))\n",
    "print(y_valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ps_ind_05_cat',\n",
       " 'ps_car_01_cat',\n",
       " 'ps_car_07_cat',\n",
       " 'ps_car_03_cat',\n",
       " 'ps_car_06_cat',\n",
       " 'ps_car_04_cat',\n",
       " 'ps_car_09_cat',\n",
       " 'ps_car_02_cat',\n",
       " 'ps_ind_02_cat',\n",
       " 'ps_car_05_cat',\n",
       " 'ps_car_08_cat',\n",
       " 'ps_ind_04_cat',\n",
       " 'ps_reg_01_plus_ps_car_02_cat',\n",
       " 'ps_reg_01_plus_ps_car_04_cat']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "7c6e4823-4e8c-4408-b961-576d469e9241",
    "_uuid": "6aa7ada2193c2e4b8a63eebda925cee5023b45b0"
   },
   "outputs": [],
   "source": [
    "# Set up folds\n",
    "K = 5\n",
    "kf = KFold(n_splits = K, random_state = 1, shuffle = True)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "5d8108f3-e9e8-45d6-93b5-740eb7b4b10b",
    "_uuid": "581c3f15f294378a0e2ac3305e9e3d375f664b21"
   },
   "outputs": [],
   "source": [
    "# Set up classifier\n",
    "model = XGBClassifier(    \n",
    "                        n_estimators=MAX_ROUNDS,\n",
    "                        max_depth=4,\n",
    "                        objective=\"binary:logistic\",\n",
    "                        learning_rate=LEARNING_RATE, \n",
    "                        subsample=.8,\n",
    "                        min_child_weight=6,\n",
    "                        colsample_bytree=.8,\n",
    "                        scale_pos_weight=1.6,\n",
    "                        gamma=10,\n",
    "                        reg_alpha=8,\n",
    "                        reg_lambda=1.3,\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     0      1      2 ... 595209 595210 595211]\n",
      "(476169,)\n",
      "\n",
      "\n",
      "[     3      8      9 ... 595201 595202 595204]\n",
      "(119043,)\n",
      "--------------------------------------------------------------------------------\n",
      "[     0      1      3 ... 595208 595209 595210]\n",
      "(476169,)\n",
      "\n",
      "\n",
      "[     2      7     11 ... 595203 595205 595211]\n",
      "(119043,)\n",
      "--------------------------------------------------------------------------------\n",
      "[     2      3      5 ... 595207 595209 595211]\n",
      "(476170,)\n",
      "\n",
      "\n",
      "[     0      1      4 ... 595200 595208 595210]\n",
      "(119042,)\n",
      "--------------------------------------------------------------------------------\n",
      "[     0      1      2 ... 595209 595210 595211]\n",
      "(476170,)\n",
      "\n",
      "\n",
      "[    10     17     21 ... 595175 595192 595196]\n",
      "(119042,)\n",
      "--------------------------------------------------------------------------------\n",
      "[     0      1      2 ... 595208 595210 595211]\n",
      "(476170,)\n",
      "\n",
      "\n",
      "[     5     12     16 ... 595206 595207 595209]\n",
      "(119042,)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i, (train_index, test_index) in enumerate(kf.split(train_df)):\n",
    "    print(train_index)\n",
    "    print(train_index.shape)\n",
    "    print('\\n')\n",
    "    print(test_index)\n",
    "    print(test_index.shape)\n",
    "    print('-'*80)\n",
    "    # kolded, shuffle = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df['ps_reg_01'].mean()\n",
    "# smoothing = 10\n",
    "# smoothing = 1 / (1 + np.exp(-(train_df['ps_calc_01'].count() - 200) / smoothing))\n",
    "# print(np.exp(-(train_df['ps_calc_01'].count() - 200)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "c4e48347-920f-4ba7-8b37-cfbaab4c3c00",
    "_uuid": "2b9ed96c98b705d3e4bf2a3d60323dfab4332674"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold  0\n",
      "Encode end\n",
      "fit start\n",
      "model fit.\n",
      "[0.02368145 0.04678843 0.06983136 ... 0.0368156  0.03348101 0.05773815]\n",
      "  Gini =  0.2849993810505098\n",
      "\n",
      "Fold  1\n",
      "Encode end\n",
      "fit start\n",
      "model fit.\n",
      "[0.03058705 0.03388928 0.04379487 ... 0.09302994 0.05414506 0.02611043]\n",
      "  Gini =  0.2819388953345532\n",
      "\n",
      "Fold  2\n",
      "Encode end\n",
      "fit start\n",
      "model fit.\n",
      "[0.06399968 0.04613879 0.06331564 ... 0.03274471 0.06336704 0.03973275]\n",
      "  Gini =  0.2763158671178396\n",
      "\n",
      "Fold  3\n",
      "Encode end\n",
      "fit start\n",
      "model fit.\n",
      "[0.05342885 0.0789933  0.0460133  ... 0.0321997  0.0434684  0.09406571]\n",
      "  Gini =  0.3022602735495421\n",
      "\n",
      "Fold  4\n",
      "Encode end\n",
      "fit start\n",
      "model fit.\n",
      "[0.05312172 0.0323148  0.04120189 ... 0.05744714 0.03252183 0.0249834 ]\n",
      "  Gini =  0.2840475033555092\n"
     ]
    }
   ],
   "source": [
    "# Run CV\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(train_df)):\n",
    "    \n",
    "    # Create data for this fold\n",
    "    y_train, y_valid = y.iloc[train_index].copy(), y.iloc[test_index] # Tuple\n",
    "    X_train, X_valid = X.iloc[train_index,:].copy(), X.iloc[test_index,:].copy() # pandas.Series\n",
    "    X_test = test_df.copy()\n",
    "    print( \"\\nFold \", i)\n",
    "    \n",
    "    # Encode data\n",
    "    for f in f_cats:\n",
    "        X_train[f + \"_avg\"], X_valid[f + \"_avg\"], X_test[f + \"_avg\"] = target_encode(\n",
    "                                                        trn_series=X_train[f],\n",
    "                                                        val_series=X_valid[f],\n",
    "                                                        tst_series=X_test[f],\n",
    "                                                        target=y_train,\n",
    "                                                        min_samples_leaf=200,\n",
    "                                                        smoothing=10,\n",
    "                                                        noise_level=0\n",
    "                                                        )\n",
    "    print('Encode end')\n",
    "    # Run model for this fold\n",
    "        # OPTIMIZE_ROUNS is default == False\n",
    "    if OPTIMIZE_ROUNDS:\n",
    "        eval_set=[(X_valid,y_valid)]\n",
    "        fit_model = model.fit( X_train, y_train, \n",
    "                               eval_set=eval_set,\n",
    "                               eval_metric=gini_xgb,\n",
    "                               early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "                               verbose=False\n",
    "                             )\n",
    "        print( \"  Best N trees = \", model.best_ntree_limit )\n",
    "        print( \"  Best gini = \", model.best_score )\n",
    "    else:\n",
    "        print('fit start')\n",
    "        fit_model = model.fit( X_train, y_train , verbose=True)\n",
    "        print('model fit.')\n",
    "        \n",
    "    # Generate validation predictions for this fold\n",
    "    pred = fit_model.predict_proba(X_valid)[:,1]\n",
    "    print(pred)\n",
    "    print( \"  Gini = \", eval_gini(y_valid, pred) )\n",
    "    y_valid_pred.iloc[test_index] = pred\n",
    "    # Accumulate test set predictions\n",
    "    y_test_pred += fit_model.predict_proba(X_test)[:,1]\n",
    "        # y_test_pred < for submit.csv file\n",
    "    del X_test, X_train, X_valid, y_train\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gini for full training set:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.28568575853594425"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "y_test_pred /= K  # Average test set predictions\n",
    "\n",
    "print( \"\\nGini for full training set:\" )\n",
    "eval_gini(y, y_valid_pred)\n",
    "\n",
    "# Run CV end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_eval_gini(y_true, y_prob):\n",
    "    y_true = np.asarray(y_true)\n",
    "    print(Counter(y_true))\n",
    "    print('type is',type(y_true))\n",
    "    y_true = y_true[np.argsort(y_prob)]\n",
    "        # Based on y_prob, return indexes in small order\n",
    "    ntrue = 0\n",
    "    gini = 0\n",
    "    delta = 0\n",
    "    n = len(y_true)\n",
    "    print('len is ', n)\n",
    "    for i in range(n-1, -1, -1):\n",
    "        y_i = y_true[i]\n",
    "        ntrue += y_i\n",
    "        gini += y_i * delta\n",
    "        delta += 1 - y_i\n",
    "        # if y value is contain '1',\n",
    "            # gini value is improve ++delta values\n",
    "            # and delta value is retained\n",
    "        # if y value is contain '0',\n",
    "            # delta values improve ++1,\n",
    "    \n",
    "    print('true count : ', ntrue)\n",
    "    print('gini is : ', gini)\n",
    "    # print('delta value is change : ', delta)\n",
    "    print('ntrue * (n-nture) is : ',ntrue*(n-ntrue))\n",
    "    gini = 1 - (2 * gini) / (ntrue * (n - ntrue))\n",
    "    # gini = 1- ()\n",
    "    return gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 573518, 1: 21694})\n",
      "type is <class 'numpy.ndarray'>\n",
      "len is  595212\n",
      "true count :  21694\n",
      "gini is :  4443712999\n",
      "ntrue * (n-nture) is :  12441899492\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.28568575853594425"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_eval_gini(y, y_valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 573518, 1: 21694})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "_cell_guid": "0e3dfd76-c566-4b8d-a460-b56e964d0772",
    "_uuid": "e61bf4e22c1c29c8358caeecb6e67d6658f2005d"
   },
   "outputs": [],
   "source": [
    "# Save validation predictions for stacking/ensembling\n",
    "val = pd.DataFrame()\n",
    "val['id'] = id_train\n",
    "val['target'] = y_valid_pred.values\n",
    "val.to_csv('./result/xgb_valid.csv', float_format='%.6f', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "_cell_guid": "f4cbef2c-e52b-4afb-b8ef-904ee9b5f9d5",
    "_uuid": "380fc8053d00cd8bb2796bfd2b59d10cbc4ce7e1"
   },
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "sub = pd.DataFrame()\n",
    "sub['id'] = id_test\n",
    "sub['target'] = y_test_pred\n",
    "sub.to_csv('./result/xgb_submit.csv', float_format='%.6f', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "77516718-78a9-4043-8b4b-0b04276e4345",
    "_uuid": "5401495c2c34ef736c761573c70d7e3b4efa3a5b"
   },
   "source": [
    "Notes:<br>\n",
    "version 16. Baseline best CV=.2832, LB=.282<br>\n",
    "version 15. Ntree optimization for baseline<br>\n",
    "version 21. Verbose version of baseline optimization<br>\n",
    "version 22. Baseline + per-fold early stopping after 20 rounds<br>\n",
    "version 23. Back to baseline.<br>\n",
    "version 24. Some parameter tuning.<br>\n",
    "version 25. Re-published to make it visible.<br>\n",
    "version 26. A little more tuning.<br>\n",
    "version 27: More tuning, get rid of upsampling (using  **<code>scale_pos_weight</code>** instead),<br>\n",
    "                    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "                    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "                    Set <code>OPTIMIZE_ROUNDS</code> and <code>verbose</code> temporarily<br>\n",
    "version 28: <code>MAX_ROUNDS=300</code> as a compromise<br>\n",
    "version 29: Substantively identical. (Turn off now-irrelevant <code>verbose</code>.)<br>\n",
    "version 30: Still substantively identical. Some visual cleanup.<br>\n",
    "version 35. More tuning. CV went up but LB sorts lower (still .283)<br>\n",
    "version 36. Identical (except turn off irrelevant <code>verbose</code>). Republished to make it visible.<br>\n",
    "versions 37-42. More tuning (gamma=10, alpha=8). LB .284 (\\*end zone dance\\*).<br>\n",
    "version 43. More tuning (min_child_weight=6).  LB score has considerably improved according to sort, but still .284"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
